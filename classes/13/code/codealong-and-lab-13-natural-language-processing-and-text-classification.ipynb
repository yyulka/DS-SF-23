{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-SF-23 | Codealong and Lab 13 | Natural Language Processing and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codealong - Text Processing with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_extraction, ensemble, cross_validation, metrics\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is about sentiments on Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentiments = []\n",
    "\n",
    "with open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        review, sentiment = line.split('\\t')\n",
    "        sentiment = np.nan if sentiment == '' else int(sentiment)\n",
    "\n",
    "        reviews.append(review)\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True) # Let's drop NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.review\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = cross_validation.train_test_split(X, y, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` converts a collection of text into a matrix of features.  Each row will be a sample (an article or piece of text) and each column will be a text feature (usually a count or binary feature per word).\n",
    "\n",
    "`CountVectorizer` takes a column of text and creates a new dataset.  It generates a feature for every word in all of the pieces of text.\n",
    "\n",
    "CAUTION: Using all of the words can be useful, but we may need to use regularization to avoid overfitting.  Otherwise, rare words may cause the model to overfit and not generalize.\n",
    "\n",
    "(And check http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a new CountVectorizer\n",
    "\n",
    "# Stopwords are non-content words.  (e.g. 'to', 'the', and 'it')\n",
    "# They arenâ€™t helpful for prediction, so we remove them.\n",
    "\n",
    "vectorizer = feature_extraction.text.CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizers are like other models in `sklearn`:\n",
    "- We create a vectorizer object with the parameters of our feature space\n",
    "- We fit a vectorizer to learn the vocabulary\n",
    "- We transform a set of text into that feature space\n",
    "\n",
    "Note: there is a distinction between fit and transform:\n",
    "- We fit from our training set.  This is part of the model building process, so we don't look at our test set\n",
    "- We transform our test set using our model fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use `fit` to learn the vocabulary of the reviews\n",
    "\n",
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The bag-of-words\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use `tranform` to generate the sample X word matrix - one column per feature (here, a word)\n",
    "\n",
    "train_X_transformed = vectorizer.transform(train_X)\n",
    "test_X_transformed = vectorizer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While dense matrices store every entry in the matrix, sparse matrices only store the nonzero entries.  Sparse matrices don't have a lot of extra features, and some algorithms may not work for them so you use them when you need to work with matrices that would be too big for the computer to handle them, but they are mostly zero, so they compress easily.  You can convert from sparse matrices to dense matrices with `.todense()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X_transformed.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a random forest model to predict \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "cross_validation.cross_val_score(model, train_X_transformed, train_y, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_X_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(train_X_transformed, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_hat = model.predict(train_X_transformed)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(train_y, train_y_hat)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label = 'ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([.0, 1.])\n",
    "plt.ylim([.0, 1.1])\n",
    "plt.xlabel('FPR/Fall-out')\n",
    "plt.ylabel('TPR/Sensitivity')\n",
    "plt.title('Testing Sentiment ROC')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(test_X_transformed, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_y_hat = model.predict(test_X_transformed)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, test_y_hat)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label = 'ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([.0, 1.])\n",
    "plt.ylim([.0, 1.1])\n",
    "plt.xlabel('FPR/Fall-out')\n",
    "plt.ylabel('TPR/Sensitivity')\n",
    "plt.title('Training Sentiment ROC')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Directions: Redo the analysis above with `TfidfVectorizer` instead of `CountVectorizer`.  What do you get?\n",
    "\n",
    "(Check http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
